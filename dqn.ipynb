{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Initial imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "\r\n",
    "import time\r\n",
    "import gym\r\n",
    "import torch\r\n",
    "import random\r\n",
    "from collections import defaultdict, deque\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import torch.nn.functional as F\r\n",
    "from collections import namedtuple, deque\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import copy\r\n",
    "from pprint import pprint\r\n",
    "import math\r\n",
    "from tqdm.autonotebook import tqdm\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from torchvision import transforms\r\n",
    "from joblib import Parallel, delayed\r\n",
    "import gc\r\n",
    "import gym_joinemio\r\n",
    "from gym_joinemio.envs.player import RandomPlayer"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Neptune prep"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "# import neptune\r\n",
    "# import neptune.new as neptune\r\n",
    "# import os\r\n",
    "\r\n",
    "# proj = 'jmolais/joinemio'\r\n",
    "# token = os.getenv('JOINEMIO_TOKEN')\r\n",
    "# run = neptune.init(project=proj,\r\n",
    "#                    api_token=token)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "env = gym.make('joinemio-v0')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "print(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Replay memory"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "Transition = namedtuple('Transition',\r\n",
    "                        ('state', 'action', 'next_state', 'reward'))\r\n",
    "\r\n",
    "class ReplayMemory(object):\r\n",
    "    def __init__(self, capacity):\r\n",
    "        self.memory = deque([],maxlen=capacity)\r\n",
    "\r\n",
    "    def push(self, *args):\r\n",
    "        \"\"\"Save a transition\"\"\"\r\n",
    "        self.memory.append(Transition(*args))\r\n",
    "\r\n",
    "    def sample(self, batch_size):\r\n",
    "        return random.sample(self.memory, batch_size) # TODO: Batch size delete\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.memory)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training: \r\n",
    "Random vs Random. Deep Q-Learning. Params:\r\n",
    "\r\n",
    "- `n_episodes` (int): maximum number of training epsiodes\r\n",
    "- `max_t` (int): maximum number of timesteps per episode _// Not used, because these episodes don't take too long and we like when game's are finished_\r\n",
    "- `eps_start` (float): starting value of epsilon, for epsilon-greedy action selection\r\n",
    "- `eps_end` (float): minimum value of epsilon \r\n",
    "- `eps_decay` (float): mutiplicative factor (per episode) for decreasing epsilon"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "# agent1 = RandomPlayer() # Player1\r\n",
    "# agent2 = RandomPlayer() # Player2\r\n",
    "\r\n",
    "# def dqn(n_episodes= EPISODES, eps_start=EPS_START, eps_end = EPS_END, eps_decay=EPS_DECAY):\r\n",
    "#     memory_buffer = ReplayMemory(BUFFER_SIZE)\r\n",
    "    \r\n",
    "#     scores = []                         # list containing score from each episode\r\n",
    "#     scores_window = deque(maxlen=100)   # last 100 scores\r\n",
    "#     eps = eps_start\r\n",
    "\r\n",
    "#     for i_episode in range(1, n_episodes+1):\r\n",
    "#         print(f\"=== EPISODE {i_episode} ===\")\r\n",
    "#         state = env.reset()\r\n",
    "#         score = 0\r\n",
    "        \r\n",
    "#         # # OPT A:\r\n",
    "#         # one_game = env.play_one_game(agent1, agent2, each_step_render=False)\r\n",
    "#         # for e in env.recording: print(e)\r\n",
    "\r\n",
    "#         # OPT B:\r\n",
    "#         players = [agent1, agent2]\r\n",
    "#         env.observation_space = env.reset()\r\n",
    "#         action = None\r\n",
    "        \r\n",
    "#         while not env.game.game_state == gym_joinemio.envs.board.GameState.finished:\r\n",
    "#             current_player = env.game.current_player - 1\r\n",
    "#             action = players[current_player].get_action(env.observation_space)\r\n",
    "#             state1 = env.game.board.grid\r\n",
    "#             env.observation_space, reward, done, info = env.step(action)\r\n",
    "#             state2 = env.game.board.grid\r\n",
    "\r\n",
    "#             # Recording data:\r\n",
    "#             if (current_player == 1): # (?) Do we do it for both players?\r\n",
    "#                 memory_buffer.push(state1, action, state2, reward.value) \r\n",
    "\r\n",
    "#             score +=  reward.value\r\n",
    "#             scores.append(score)\r\n",
    "#             scores_window.append(score)\r\n",
    "#             env.recording.append((current_player + 1, action, env.rewarder()))\r\n",
    "            \r\n",
    "#             # Analyzing scores\r\n",
    "#             eps = max(eps*eps_decay,eps_end)\r\n",
    "#             print('Episode {}\\tAverage Score {:.2f}\\n'.format(i_episode,np.mean(scores_window)), end=\"\")\r\n",
    "#             if np.mean(scores_window)>=200.0:\r\n",
    "#                 print('\\nEnvironment solve in {:d} epsiodes!\\tAverage score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\r\n",
    "#                 torch.save(agent.qnetwork_local.state_dict(),'checkpoint.pth')\r\n",
    "#                 break\r\n",
    "        \r\n",
    "#         print(f\"--------\\nWinner: {env.game.winner}\\n\")\r\n",
    "#         # return env.observation_space, reward, done, info  # reward for player1\r\n",
    "#     return scores\r\n",
    "\r\n",
    "# scores = dqn()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "BUFFER_SIZE = 200\r\n",
    "EPISODES = 10\r\n",
    "EPS_START = 1.0\r\n",
    "EPS_END = 0.01\r\n",
    "EPS_DECAY = 0.996\r\n",
    "EPS_DECAY_LAST_FRAME = 10**5\r\n",
    "LEARNING_RATE = 1e-4\r\n",
    "\r\n",
    "MEAN_REWARD_BOUND = 20 # TODO evaluate proper default val\r\n",
    "\r\n",
    "\r\n",
    "# GAMMA = 0.99\r\n",
    "# BATCH_SIZE = 32\r\n",
    "# REPLAY_SIZE = 10000\r\n",
    "# LEARNING_RATE = 1e-4\r\n",
    "SYNC_TARGET_FRAMES = 1000\r\n",
    "REPLAY_START_SIZE = 10000\r\n",
    "\r\n",
    "# EPSILON_DECAY_LAST_FRAME = 10**5\r\n",
    "# EPSILON_START = 1.0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "from gym_joinemio.envs.board import Board, GameState\r\n",
    "\r\n",
    "\r\n",
    "class NeuralNetwork(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(NeuralNetwork, self).__init__()\r\n",
    "\r\n",
    "        self.main_layers = nn.Sequential(\r\n",
    "            nn.Linear(6*7, 6*7),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(6*7, 6*7),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(6*7, 6*7),\r\n",
    "            nn.ReLU()\r\n",
    "        )\r\n",
    "        self.output_layer = nn.Linear(6*7,7)\r\n",
    "\r\n",
    "    def forward(self, board_flatten_state):\r\n",
    "        # x1 = torch.tensor(input.astype(float)).type(torch.FloatTensor)\r\n",
    "        # x = torch.flatten(x1)\r\n",
    "        # for layer in self.net:\r\n",
    "        #     x = layer(x)\r\n",
    "        weights = self.output_layer(self.main_layers(board_flatten_state)) # weights = nn.relU(self.output_layer(self.main_layers(board_flatten_state)))\r\n",
    "        return weights\r\n",
    "\r\n",
    "class AIPlayer:\r\n",
    "    @staticmethod\r\n",
    "    def possible_moves(board_state):\r\n",
    "        available_cols = []\r\n",
    "        for i in range(len(board_state[0])):\r\n",
    "            if board_state[0][i] == 0:\r\n",
    "                available_cols.append(i)\r\n",
    "        return available_cols\r\n",
    "\r\n",
    "    def __init__(self, env, replay_memory):\r\n",
    "        # self.net = NeuralNetwork()\r\n",
    "        self.env = env\r\n",
    "        self.env.opponent_action_set(RandomPlayer.get_action)\r\n",
    "        self.replay_memory = replay_memory\r\n",
    "        self._reset()\r\n",
    "\r\n",
    "    def _reset(self):\r\n",
    "        self.env = gym.make('joinemio-v0')\r\n",
    "        self.state =  self.env.reset()\r\n",
    "        self.env.opponent_action_set(RandomPlayer.get_action)\r\n",
    "        self.env.game = gym_joinemio.envs.board.Game()\r\n",
    "        self.total_reward = 0.0\r\n",
    "\r\n",
    "    def get_action(self, board_state):\r\n",
    "        # weigths = self.net.forward(board_state)\r\n",
    "        weigths = self.net.forward(torch.flatten(torch.tensor(board_state.astype(float)).type(torch.FloatTensor)))\r\n",
    "        pos_nums = self.possible_moves(board_state)\r\n",
    "        max_num = 0\r\n",
    "        for col in pos_nums:\r\n",
    "            if weigths[max_num] < weigths[int(col)]:\r\n",
    "                max_num = int(col)\r\n",
    "        return max_num\r\n",
    "\r\n",
    "    def play_step(self, net, epsilon, device):\r\n",
    "        done_reward = None\r\n",
    "        final_reward = None\r\n",
    "        if np.random.random() < epsilon: # if epsilon > 0.6:\r\n",
    "            grid = self.env.game.board.grid\r\n",
    "            action = RandomPlayer.get_action(grid)\r\n",
    "        else: \r\n",
    "            state_a = np.array([self.state], copy=False, dtype='f')\r\n",
    "            # print(state_a.dtype)\r\n",
    "            state_v = torch.flatten(torch.tensor(state_a).to(device))\r\n",
    "            q_vals_v = net(state_v)\r\n",
    "            _, act_v = torch.max(q_vals_v, dim=0)\r\n",
    "            action = int(act_v.item())  # TODO: Later check if it returns correct action int range.\r\n",
    "\r\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\r\n",
    "        final_reward = reward\r\n",
    "        self.total_reward += reward.value\r\n",
    "\r\n",
    "        # exp = Transition(self.state, action, new_state, reward)\r\n",
    "        # self.replay_memory.push(exp)\r\n",
    "        self.replay_memory.push(self.state, action, new_state, reward)\r\n",
    "        self.state = new_state\r\n",
    "        \r\n",
    "        if is_done == GameState.finished:\r\n",
    "            done_reward = self.total_reward\r\n",
    "            print(f\"WINNER: player {self.env.game.winner} | Final reward reading: {final_reward}\")\r\n",
    "            print(self.env.game.board.grid)\r\n",
    "            self._reset()\r\n",
    "            return (done_reward, final_reward)\r\n",
    "        else: return (done_reward, None)\r\n",
    "\r\n",
    "    def train(self, memory_buffer, batch_size):\r\n",
    "        return 0\r\n",
    "\r\n",
    "def our_main():\r\n",
    "    env = gym.make('joinemio-v0')\r\n",
    "    net = NeuralNetwork().to(device)\r\n",
    "    tgt_net = NeuralNetwork().to(device)\r\n",
    "    print(net)\r\n",
    "    buffer = ReplayMemory(BUFFER_SIZE)\r\n",
    "    agent = AIPlayer(env, buffer) # TODO: params? Whatever agent is...\r\n",
    "    epsilon = EPS_START\r\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\r\n",
    "    \r\n",
    "    total_rewards = []\r\n",
    "    frame_idx = 0\r\n",
    "    ts_frame = 0\r\n",
    "    ts = time.time()\r\n",
    "    best_mean_reward = None\r\n",
    "    last_total_frames = 0\r\n",
    "    while True:\r\n",
    "        frame_idx += 1\r\n",
    "        epsilon = max(EPS_END, EPS_START - frame_idx / EPS_DECAY_LAST_FRAME)\r\n",
    "        reward, final_reward = agent.play_step(net, epsilon, device=device)\r\n",
    "        if (reward is not None):\r\n",
    "            frames_this_game = frame_idx - last_total_frames\r\n",
    "            last_total_frames = frame_idx\r\n",
    "            print(\"Reward is not None (Game Finished)\")\r\n",
    "            total_rewards.append(reward)\r\n",
    "            ts_frame = frame_idx\r\n",
    "            ts = time.time()\r\n",
    "            mean_reward = np.mean(total_rewards[-100:])\r\n",
    "            print(f'Total Steps Played: {frame_idx}, frames this game {frames_this_game}, Games done: {len(total_rewards)}, mean reward: {mean_reward}, eps: {epsilon}, final reward: {final_reward}')\r\n",
    "            print(\"\")\r\n",
    "            # TODO: # Neptun logging (write epsilon, speed, reward_100, reward)\r\n",
    "\r\n",
    "            if best_mean_reward is None or best_mean_reward < mean_reward:\r\n",
    "                torch.save(net.state_dict(), \"joinemio-best.dat\") # TODO: Extract\r\n",
    "                if best_mean_reward is not None:\r\n",
    "                    print(f\"Best mean reward updated {best_mean_reward}->{mean_reward}; model saved\")\r\n",
    "                best_mean_reward = mean_reward\r\n",
    "            if mean_reward > MEAN_REWARD_BOUND:\r\n",
    "                print(f\"Solved in {frame_idx} frames\")\r\n",
    "                break\r\n",
    "        else: print(\"REWARD NONE\")\r\n",
    "        \r\n",
    "        if len(buffer) < REPLAY_START_SIZE:\r\n",
    "            continue\r\n",
    "\r\n",
    "        if frame_idx % SYNC_TARGET_FRAMES == 0:\r\n",
    "            tgt_net.load_state_dict(net.state_dict())\r\n",
    "\r\n",
    "        optimizer.zero_grad()\r\n",
    "        batch = buffer.sample(BATCH_SIZE)\r\n",
    "        # loss_t = calc_loss(batch, net, tgt_net, device=device) # TODO: calc_loss method\r\n",
    "        optimizer.step()\r\n",
    "\r\n",
    "our_main()\r\n",
    "\r\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NeuralNetwork(\n",
      "  (main_layers): Sequential(\n",
      "    (0): Linear(in_features=42, out_features=42, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=42, out_features=42, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=42, out_features=42, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (output_layer): Linear(in_features=42, out_features=7, bias=True)\n",
      ")\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 1 0 2 1 2]\n",
      " [0 0 2 0 1 1 2]\n",
      " [0 1 2 1 1 2 2]\n",
      " [2 2 2 1 1 1 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 11, frames this game 11, Games done: 1, mean reward: -10.238095238095237, eps: 0.99989, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 1 0 0 0 1 1]\n",
      " [2 2 2 2 0 2 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 16, frames this game 5, Games done: 2, mean reward: -10.166666666666666, eps: 0.99984, final reward: Reward.loss\n",
      "\n",
      "Best mean reward updated -10.238095238095237->-10.166666666666666; model saved\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 0 1 0 0]\n",
      " [1 0 0 0 2 0 2]\n",
      " [1 0 0 2 2 0 1]\n",
      " [2 0 0 2 1 2 1]\n",
      " [2 1 0 2 2 1 2]\n",
      " [1 1 2 2 1 1 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 29, frames this game 13, Games done: 3, mean reward: -10.206349206349207, eps: 0.99971, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 1 2 0 2 0 0]\n",
      " [0 2 2 0 2 1 0]\n",
      " [1 1 1 1 2 1 2]\n",
      " [1 1 2 2 1 2 1]\n",
      " [2 2 1 2 1 1 2]\n",
      " [1 2 1 1 2 1 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 47, frames this game 18, Games done: 4, mean reward: -7.505952380952381, eps: 0.99953, final reward: Reward.win\n",
      "\n",
      "Best mean reward updated -10.166666666666666->-7.505952380952381; model saved\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 2]\n",
      " [2 1 0 0 1 0 2]\n",
      " [1 2 0 1 1 0 2]\n",
      " [2 2 0 1 2 0 2]\n",
      " [1 2 1 1 1 2 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 58, frames this game 11, Games done: 5, mean reward: -8.052380952380952, eps: 0.99942, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[1 2 1 0 1 2 1]\n",
      " [1 2 2 0 2 1 1]\n",
      " [1 1 1 2 2 1 1]\n",
      " [2 2 2 1 2 2 2]\n",
      " [1 2 1 2 1 1 2]\n",
      " [1 2 1 1 2 2 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 78, frames this game 20, Games done: 6, mean reward: -8.45238095238095, eps: 0.99922, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 2 0 0 2 0]\n",
      " [0 2 1 1 1 1 2]\n",
      " [1 2 1 1 1 2 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 86, frames this game 8, Games done: 7, mean reward: -7.125850340136053, eps: 0.99914, final reward: Reward.win\n",
      "\n",
      "Best mean reward updated -7.505952380952381->-7.125850340136053; model saved\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 0 2 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 2 2 0 0]\n",
      " [0 1 2 2 2 0 2]\n",
      " [1 1 1 2 1 0 2]\n",
      " [2 1 1 2 1 1 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 97, frames this game 11, Games done: 8, mean reward: -7.5148809523809526, eps: 0.99903, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 2 2 2 2 1 0]\n",
      " [1 1 2 1 2 2 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 105, frames this game 8, Games done: 9, mean reward: -7.80952380952381, eps: 0.99895, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[2 2 1 0 0 0 0]\n",
      " [1 1 2 0 0 0 0]\n",
      " [2 1 1 1 1 0 0]\n",
      " [2 2 1 2 2 0 1]\n",
      " [2 1 1 2 2 1 1]\n",
      " [1 1 2 2 2 1 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 121, frames this game 16, Games done: 10, mean reward: -6.964285714285715, eps: 0.99879, final reward: Reward.win\n",
      "\n",
      "Best mean reward updated -7.125850340136053->-6.964285714285715; model saved\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 2 0]\n",
      " [0 0 0 1 0 1 0]\n",
      " [0 0 0 1 2 1 0]\n",
      " [0 0 2 2 2 2 0]\n",
      " [1 2 2 2 1 1 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 130, frames this game 9, Games done: 11, mean reward: -7.257575757575759, eps: 0.9987, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 2 0 0 0]\n",
      " [0 0 2 1 2 0 0]\n",
      " [2 0 2 1 1 0 2]\n",
      " [2 1 1 1 1 0 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 139, frames this game 9, Games done: 12, mean reward: -6.585317460317461, eps: 0.99861, final reward: Reward.win\n",
      "\n",
      "Best mean reward updated -6.964285714285715->-6.585317460317461; model saved\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 1]\n",
      " [0 0 0 0 2 0 1]\n",
      " [1 2 0 0 2 0 1]\n",
      " [2 1 0 2 2 0 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 146, frames this game 7, Games done: 13, mean reward: -6.012820512820513, eps: 0.99854, final reward: Reward.win\n",
      "\n",
      "Best mean reward updated -6.585317460317461->-6.012820512820513; model saved\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 2 0 0 0]\n",
      " [0 1 1 1 1 2 0]\n",
      " [1 2 1 2 1 2 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 153, frames this game 7, Games done: 14, mean reward: -5.522108843537415, eps: 0.99847, final reward: Reward.win\n",
      "\n",
      "Best mean reward updated -6.012820512820513->-5.522108843537415; model saved\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 2 1 1 0 2]\n",
      " [0 0 2 2 1 2 2]\n",
      " [0 0 1 1 2 2 2]\n",
      " [0 0 1 2 1 1 1]\n",
      " [2 0 2 1 2 2 1]\n",
      " [1 1 1 2 1 1 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 169, frames this game 16, Games done: 15, mean reward: -5.844444444444445, eps: 0.99831, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 0 0 2 0]\n",
      " [0 0 0 0 0 2 0]\n",
      " [0 1 2 0 0 1 0]\n",
      " [2 2 1 0 2 2 2]\n",
      " [1 1 1 1 1 2 1]\n",
      " [1 2 1 1 1 2 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 182, frames this game 13, Games done: 16, mean reward: -5.434523809523809, eps: 0.99818, final reward: Reward.win\n",
      "\n",
      "Best mean reward updated -5.522108843537415->-5.434523809523809; model saved\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 2 0 0]\n",
      " [2 0 0 0 1 2 0]\n",
      " [1 1 1 1 2 2 0]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 188, frames this game 6, Games done: 17, mean reward: -5.063025210084033, eps: 0.99812, final reward: Reward.win\n",
      "\n",
      "Best mean reward updated -5.434523809523809->-5.063025210084033; model saved\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 1 0 0 0]\n",
      " [1 2 0 1 0 0 0]\n",
      " [1 2 0 2 2 0 0]\n",
      " [2 1 0 1 2 1 2]\n",
      " [1 1 2 2 2 1 2]\n",
      " [2 1 2 1 2 1 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 202, frames this game 14, Games done: 18, mean reward: -5.354497354497354, eps: 0.99798, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 2 0 2 0]\n",
      " [0 1 0 2 2 1 0]\n",
      " [0 2 1 2 1 1 0]\n",
      " [1 1 2 1 2 1 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 211, frames this game 9, Games done: 19, mean reward: -5.609022556390977, eps: 0.99789, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 2 2 0 1]\n",
      " [0 0 0 1 2 0 1]\n",
      " [2 0 1 2 1 2 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 218, frames this game 7, Games done: 20, mean reward: -5.285714285714286, eps: 0.99782, final reward: Reward.win\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 2 0 0]\n",
      " [1 1 0 0 2 1 0]\n",
      " [2 2 0 2 2 1 0]\n",
      " [1 1 2 1 2 1 0]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 226, frames this game 8, Games done: 21, mean reward: -5.518140589569161, eps: 0.99774, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 1 0 0 2]\n",
      " [0 0 0 1 1 0 2]\n",
      " [0 0 1 2 2 0 1]\n",
      " [0 0 1 2 1 2 1]\n",
      " [1 2 2 2 2 1 2]\n",
      " [2 1 1 1 2 2 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 240, frames this game 14, Games done: 22, mean reward: -5.735930735930736, eps: 0.9976, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 0 2 0 0]\n",
      " [0 0 0 2 1 0 1]\n",
      " [0 0 0 2 2 2 2]\n",
      " [0 0 0 1 1 1 2]\n",
      " [2 1 0 1 1 1 2]\n",
      " [2 2 0 2 1 1 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 252, frames this game 12, Games done: 23, mean reward: -5.932712215320912, eps: 0.99748, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 0 1 0 0]\n",
      " [0 0 0 0 2 0 1]\n",
      " [0 1 0 1 1 0 2]\n",
      " [0 2 0 2 2 2 2]\n",
      " [1 2 1 1 1 2 2]\n",
      " [2 2 1 1 1 2 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 265, frames this game 13, Games done: 24, mean reward: -6.114087301587301, eps: 0.99735, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 0 0 0 0]\n",
      " [1 2 0 2 0 0 1]\n",
      " [2 1 0 2 1 0 2]\n",
      " [2 2 1 2 1 0 1]\n",
      " [1 2 1 1 1 2 2]\n",
      " [1 1 2 1 2 2 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 280, frames this game 15, Games done: 25, mean reward: -5.842857142857143, eps: 0.9972, final reward: Reward.win\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 2 0 0 0]\n",
      " [0 0 0 2 0 2 2]\n",
      " [1 0 0 1 2 1 1]\n",
      " [1 0 0 2 1 2 2]\n",
      " [1 1 2 2 1 1 1]\n",
      " [2 1 1 2 1 2 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 294, frames this game 14, Games done: 26, mean reward: -6.0146520146520155, eps: 0.99706, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[2 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [2 0 0 0 0 0 0]\n",
      " [2 0 2 0 0 0 2]\n",
      " [1 0 2 1 1 0 1]\n",
      " [1 1 1 1 2 0 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 303, frames this game 9, Games done: 27, mean reward: -5.761904761904764, eps: 0.99697, final reward: Reward.win\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 0 2 0 0]\n",
      " [0 0 0 0 2 0 0]\n",
      " [2 0 0 1 2 0 0]\n",
      " [2 0 0 2 1 0 0]\n",
      " [1 0 2 1 1 1 2]\n",
      " [2 1 1 1 2 1 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 314, frames this game 11, Games done: 28, mean reward: -5.528911564625852, eps: 0.99686, final reward: Reward.win\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [1 0 0 2 2 0 0]\n",
      " [1 1 2 2 1 0 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 320, frames this game 6, Games done: 29, mean reward: -5.307881773399016, eps: 0.9968, final reward: Reward.win\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 2 0 0 0]\n",
      " [0 0 0 2 0 0 0]\n",
      " [0 0 0 2 0 2 1]\n",
      " [0 1 0 2 0 2 1]\n",
      " [1 1 0 1 1 2 1]\n",
      " [2 2 1 2 1 1 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 331, frames this game 11, Games done: 30, mean reward: -5.472222222222224, eps: 0.99669, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 2]\n",
      " [0 0 0 1 0 0 2]\n",
      " [2 0 2 1 1 1 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 336, frames this game 5, Games done: 31, mean reward: -5.266513056835639, eps: 0.99664, final reward: Reward.win\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [2 0 1 0 0 0 0]\n",
      " [1 1 1 0 0 2 2]\n",
      " [2 1 1 0 0 1 2]\n",
      " [2 2 1 2 1 1 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 346, frames this game 10, Games done: 32, mean reward: -5.0773809523809526, eps: 0.99654, final reward: Reward.win\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 0 2 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [1 1 0 0 2 2 0]\n",
      " [2 2 1 0 1 2 0]\n",
      " [1 1 2 1 2 1 0]\n",
      " [2 2 1 2 1 1 0]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 358, frames this game 12, Games done: 33, mean reward: -4.901154401154401, eps: 0.99642, final reward: Reward.win\n",
      "\n",
      "Best mean reward updated -5.063025210084033->-4.901154401154401; model saved\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 0 0]\n",
      " [1 2 0 2 1 0 0]\n",
      " [1 2 1 2 2 0 0]\n",
      " [2 2 1 1 1 2 0]\n",
      " [2 1 2 2 1 1 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 370, frames this game 12, Games done: 34, mean reward: -5.0588235294117645, eps: 0.9963, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 1 0]\n",
      " [2 2 2 2 0 1 0]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 374, frames this game 4, Games done: 35, mean reward: -5.202040816326531, eps: 0.99626, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 1 1 0 0]\n",
      " [0 0 0 1 1 0 0]\n",
      " [0 0 0 2 1 2 0]\n",
      " [0 2 2 2 2 1 1]\n",
      " [2 2 2 1 1 1 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 384, frames this game 10, Games done: 36, mean reward: -5.341269841269842, eps: 0.99616, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 2]\n",
      " [0 0 0 2 0 0 2]\n",
      " [0 0 0 2 0 2 1]\n",
      " [1 1 1 1 1 1 2]\n",
      " [2 1 2 2 1 2 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 395, frames this game 11, Games done: 37, mean reward: -5.176319176319177, eps: 0.99605, final reward: Reward.win\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 2 1 0 0 0 0]\n",
      " [0 2 2 0 1 0 0]\n",
      " [0 2 1 0 2 2 0]\n",
      " [0 2 1 2 1 1 1]\n",
      " [1 1 2 2 1 2 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 406, frames this game 11, Games done: 38, mean reward: -5.30952380952381, eps: 0.99594, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 2 0 0 0 0 1]\n",
      " [2 2 2 0 1 1 1]\n",
      " [2 2 1 0 1 2 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 414, frames this game 8, Games done: 39, mean reward: -5.152014652014652, eps: 0.99586, final reward: Reward.win\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 0 2 0 0]\n",
      " [0 0 2 0 1 0 0]\n",
      " [0 0 1 2 2 0 2]\n",
      " [1 0 1 2 1 0 2]\n",
      " [2 1 1 2 2 2 1]\n",
      " [1 1 2 2 1 1 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 427, frames this game 13, Games done: 40, mean reward: -5.280357142857143, eps: 0.99573, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 2]\n",
      " [0 1 0 0 0 0 2]\n",
      " [0 1 0 0 0 0 1]\n",
      " [1 1 0 0 0 0 2]\n",
      " [2 1 1 2 2 2 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 435, frames this game 8, Games done: 41, mean reward: -5.131242740998839, eps: 0.99565, final reward: Reward.win\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 2 0 2 0]\n",
      " [0 2 1 1 1 1 0]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 439, frames this game 4, Games done: 42, mean reward: -4.986961451247166, eps: 0.99561, final reward: Reward.win\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 2 0 0 0 1 0]\n",
      " [0 2 0 2 0 1 0]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 443, frames this game 4, Games done: 43, mean reward: -4.8493909191583615, eps: 0.99557, final reward: Reward.win\n",
      "\n",
      "Best mean reward updated -4.901154401154401->-4.8493909191583615; model saved\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 1 0 0 0 0 0]\n",
      " [0 1 2 0 0 0 0]\n",
      " [0 1 2 0 0 0 0]\n",
      " [0 1 1 0 0 0 0]\n",
      " [0 2 2 2 0 0 2]\n",
      " [0 2 1 1 0 0 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 451, frames this game 8, Games done: 44, mean reward: -4.720238095238096, eps: 0.99549, final reward: Reward.win\n",
      "\n",
      "Best mean reward updated -4.8493909191583615->-4.720238095238096; model saved\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 0 2 1 0]\n",
      " [0 0 2 0 1 2 1]\n",
      " [0 0 2 0 2 1 2]\n",
      " [1 0 1 0 1 2 1]\n",
      " [2 1 2 1 1 1 1]\n",
      " [1 2 2 2 1 2 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 466, frames this game 15, Games done: 45, mean reward: -4.600529100529101, eps: 0.99534, final reward: Reward.win\n",
      "\n",
      "Best mean reward updated -4.720238095238096->-4.600529100529101; model saved\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 2 0 0 0]\n",
      " [0 2 2 1 0 1 0]\n",
      " [2 1 1 1 1 1 2]\n",
      " [1 2 2 1 2 2 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 476, frames this game 10, Games done: 46, mean reward: -4.483436853002071, eps: 0.99524, final reward: Reward.win\n",
      "\n",
      "Best mean reward updated -4.600529100529101->-4.483436853002071; model saved\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [2 0 0 0 0 0 2]\n",
      " [1 1 0 0 1 1 2]\n",
      " [1 2 0 0 1 2 2]\n",
      " [1 2 2 2 1 1 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 486, frames this game 10, Games done: 47, mean reward: -4.605369807497468, eps: 0.99514, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 2 0 2 1]\n",
      " [1 0 0 2 1 2 2]\n",
      " [1 0 0 1 2 2 2]\n",
      " [1 0 1 2 2 1 2]\n",
      " [1 2 1 1 1 2 1]\n",
      " [2 1 1 1 2 1 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 503, frames this game 17, Games done: 48, mean reward: -4.496527777777778, eps: 0.99497, final reward: Reward.win\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player None | Final reward reading: Reward.draw\n",
      "[[2 1 1 1 2 2 1]\n",
      " [2 2 1 2 1 1 1]\n",
      " [1 1 2 1 2 2 1]\n",
      " [2 1 2 1 2 2 2]\n",
      " [2 2 2 1 2 1 1]\n",
      " [1 2 1 2 1 2 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 524, frames this game 21, Games done: 49, mean reward: -4.43488824101069, eps: 0.99476, final reward: Reward.draw\n",
      "\n",
      "Best mean reward updated -4.483436853002071->-4.43488824101069; model saved\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 1 0 0 0 0 0]\n",
      " [0 2 0 0 0 0 0]\n",
      " [1 2 0 0 0 0 0]\n",
      " [1 2 1 2 0 0 0]\n",
      " [1 1 1 2 1 0 2]\n",
      " [1 2 1 2 2 1 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 535, frames this game 11, Games done: 50, mean reward: -4.330952380952381, eps: 0.99465, final reward: Reward.win\n",
      "\n",
      "Best mean reward updated -4.43488824101069->-4.330952380952381; model saved\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 2 0 0 0 0 0]\n",
      " [1 1 0 0 0 1 0]\n",
      " [1 2 0 1 0 2 0]\n",
      " [1 2 2 2 2 1 0]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 542, frames this game 7, Games done: 51, mean reward: -4.444911297852474, eps: 0.99458, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 2]\n",
      " [0 0 0 0 0 0 2]\n",
      " [0 0 0 0 0 0 2]\n",
      " [0 0 1 2 1 0 2]\n",
      " [1 0 1 2 1 0 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 548, frames this game 6, Games done: 52, mean reward: -4.554029304029304, eps: 0.99452, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 2]\n",
      " [0 1 0 2 0 0 2]\n",
      " [0 2 1 2 0 1 1]\n",
      " [1 1 1 1 0 2 2]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 556, frames this game 8, Games done: 53, mean reward: -4.452380952380952, eps: 0.99444, final reward: Reward.win\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 0 0 0 2]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 2]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 1 0 0 2 0 1]\n",
      " [2 2 2 2 1 0 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 563, frames this game 7, Games done: 54, mean reward: -4.557760141093474, eps: 0.99437, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 2 0 0 0 0 0]\n",
      " [0 2 0 1 0 1 2]\n",
      " [1 2 0 1 2 2 1]\n",
      " [1 2 1 1 2 2 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 572, frames this game 9, Games done: 55, mean reward: -4.6601731601731595, eps: 0.99428, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 1 | Final reward reading: Reward.win\n",
      "[[0 0 0 0 2 0 0]\n",
      " [0 0 0 1 2 2 0]\n",
      " [0 0 1 2 1 2 1]\n",
      " [2 0 1 1 2 1 1]\n",
      " [2 0 1 2 1 2 2]\n",
      " [2 0 1 1 2 1 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 586, frames this game 14, Games done: 56, mean reward: -4.564625850340136, eps: 0.99414, final reward: Reward.win\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 1 0 1 1 2]\n",
      " [0 2 2 0 2 2 1]\n",
      " [0 2 1 2 1 1 1]\n",
      " [2 1 2 1 2 1 2]\n",
      " [1 2 2 1 1 2 1]\n",
      " [2 2 1 2 1 2 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 604, frames this game 18, Games done: 57, mean reward: -4.667084377610693, eps: 0.99396, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "WINNER: player 2 | Final reward reading: Reward.loss\n",
      "[[0 0 0 1 0 2 0]\n",
      " [0 0 0 2 0 2 1]\n",
      " [0 0 0 1 0 2 1]\n",
      " [0 0 2 2 0 2 2]\n",
      " [2 0 1 2 1 1 1]\n",
      " [1 0 2 2 1 1 1]]\n",
      "Reward is not None (Game Finished)\n",
      "Total Steps Played: 616, frames this game 12, Games done: 58, mean reward: -4.763546798029557, eps: 0.99384, final reward: Reward.loss\n",
      "\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n",
      "REWARD NONE\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'Box'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-ed8f634f377a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m \u001b[0mour_main\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-ed8f634f377a>\u001b[0m in \u001b[0;36mour_main\u001b[1;34m()\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mframe_idx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPS_END\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPS_START\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mframe_idx\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mEPS_DECAY_LAST_FRAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mframes_this_game\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe_idx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlast_total_frames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-ed8f634f377a>\u001b[0m in \u001b[0;36mplay_step\u001b[1;34m(self, net, epsilon, device)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomPlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[0mstate_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'f'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;31m# print(state_a.dtype)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mstate_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'Box'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    " "
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d71e197be79a8854e36fe80d7b81a808575f897a047ec71b5b71be66361356a"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "8d71e197be79a8854e36fe80d7b81a808575f897a047ec71b5b71be66361356a"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}