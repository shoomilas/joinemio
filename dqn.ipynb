{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-ecd6b6545aee>, line 20)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-ecd6b6545aee>\"\u001b[1;36m, line \u001b[1;32m20\u001b[0m\n\u001b[1;33m    from gym-joinemio.gym_joinemio.envs.player import RandomPlayer\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gym\n",
    "import torch\n",
    "import random\n",
    "from collections import defaultdict, deque\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "from pprint import pprint\n",
    "import math\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from joblib import Parallel, delayed\n",
    "import gc\n",
    "import gym_joinemio\n",
    "from gym_joinemio.envs.player import RandomPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('joinemio-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\r\n",
    "                        ('state', 'action', 'next_state', 'reward'))\r\n",
    "\r\n",
    "class ReplayMemory(object):\r\n",
    "    def __init__(self, capacity):\r\n",
    "        self.memory = deque([],maxlen=capacity)\r\n",
    "\r\n",
    "    def push(self, *args):\r\n",
    "        \"\"\"Save a transition\"\"\"\r\n",
    "        self.memory.append(Transition(*args))\r\n",
    "\r\n",
    "    def sample(self, batch_size):\r\n",
    "        return random.sample(self.memory, batch_size) # TODO: Batch size delete\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\r\n",
    "    def __init__(self, h, w, outputs):    # TODO: Change params here. Different input, outputs - action space size\r\n",
    "        super(DQN, self).__init__()\r\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\r\n",
    "        self.bn1 = nn.BatchNorm2d(16)\r\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\r\n",
    "        self.bn2 = nn.BatchNorm2d(32)\r\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, sEtride=2)\r\n",
    "        self.bn3 = nn.BatchNorm2d(32)\r\n",
    "\r\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\r\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\r\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\r\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\r\n",
    "        linear_input_size = convw * convh * 32\r\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\r\n",
    "\r\n",
    "    # Called with either one element to determine next action, or a batch\r\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\r\n",
    "    def forward(self, x):\r\n",
    "        x = x.to(device)\r\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\r\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\r\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\r\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: \r\n",
    "Random vs Random. Deep Q-Learning. Params:\r\n",
    "\r\n",
    "- `n_episodes` (int): maximum number of training epsiodes\r\n",
    "- `max_t` (int): maximum number of timesteps per episode _// Not used, because these episodes don't take too long and we like when game's are finished_\r\n",
    "- `eps_start` (float): starting value of epsilon, for epsilon-greedy action selection\r\n",
    "- `eps_end` (float): minimum value of epsilon \r\n",
    "- `eps_decay` (float): mutiplicative factor (per episode) for decreasing epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 200\r\n",
    "EPISODES = 10\r\n",
    "EPS_START = 1.0\r\n",
    "EPS_END = 0.01\r\n",
    "EPS_DECAY = 0.996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EPISODE 1 ===\n",
      "Episode 1\tAverage Score 0.02\n",
      "Episode 1\tAverage Score 0.04\n",
      "Episode 1\tAverage Score 0.05\n",
      "Episode 1\tAverage Score 0.06\n",
      "Episode 1\tAverage Score 0.07\n",
      "Episode 1\tAverage Score 0.08\n",
      "Episode 1\tAverage Score 0.10\n",
      "Episode 1\tAverage Score 0.11\n",
      "Episode 1\tAverage Score 0.12\n",
      "Episode 1\tAverage Score 0.13\n",
      "Episode 1\tAverage Score 0.14\n",
      "Episode 1\tAverage Score 0.15\n",
      "Episode 1\tAverage Score 0.17\n",
      "Episode 1\tAverage Score 0.18\n",
      "Episode 1\tAverage Score 0.19\n",
      "Episode 1\tAverage Score 0.20\n",
      "Episode 1\tAverage Score 0.27\n",
      "--------\n",
      "Winner: 1\n",
      "\n",
      "=== EPISODE 2 ===\n",
      "Episode 2\tAverage Score 0.26\n",
      "Episode 2\tAverage Score 0.25\n",
      "Episode 2\tAverage Score 0.24\n",
      "Episode 2\tAverage Score 0.23\n",
      "Episode 2\tAverage Score 0.23\n",
      "Episode 2\tAverage Score 0.22\n",
      "Episode 2\tAverage Score 0.22\n",
      "Episode 2\tAverage Score 0.22\n",
      "Episode 2\tAverage Score 0.22\n",
      "Episode 2\tAverage Score 0.22\n",
      "Episode 2\tAverage Score 0.22\n",
      "Episode 2\tAverage Score 0.22\n",
      "Episode 2\tAverage Score 0.23\n",
      "Episode 2\tAverage Score 0.23\n",
      "Episode 2\tAverage Score 0.23\n",
      "Episode 2\tAverage Score 0.24\n",
      "Episode 2\tAverage Score 0.24\n",
      "Episode 2\tAverage Score -0.04\n",
      "--------\n",
      "Winner: 2\n",
      "\n",
      "=== EPISODE 3 ===\n",
      "Episode 3\tAverage Score -0.04\n",
      "Episode 3\tAverage Score -0.03\n",
      "Episode 3\tAverage Score -0.03\n",
      "Episode 3\tAverage Score -0.03\n",
      "Episode 3\tAverage Score -0.02\n",
      "Episode 3\tAverage Score -0.02\n",
      "Episode 3\tAverage Score -0.02\n",
      "Episode 3\tAverage Score -0.01\n",
      "Episode 3\tAverage Score -0.01\n",
      "Episode 3\tAverage Score -0.00\n",
      "Episode 3\tAverage Score 0.01\n",
      "Episode 3\tAverage Score 0.01\n",
      "Episode 3\tAverage Score 0.02\n",
      "Episode 3\tAverage Score 0.02\n",
      "Episode 3\tAverage Score 0.03\n",
      "Episode 3\tAverage Score 0.04\n",
      "Episode 3\tAverage Score 0.04\n",
      "Episode 3\tAverage Score 0.05\n",
      "Episode 3\tAverage Score 0.06\n",
      "Episode 3\tAverage Score 0.07\n",
      "Episode 3\tAverage Score 0.07\n",
      "Episode 3\tAverage Score 0.08\n",
      "Episode 3\tAverage Score 0.09\n",
      "Episode 3\tAverage Score 0.10\n",
      "Episode 3\tAverage Score 0.11\n",
      "Episode 3\tAverage Score -0.05\n",
      "--------\n",
      "Winner: 2\n",
      "\n",
      "=== EPISODE 4 ===\n",
      "Episode 4\tAverage Score -0.05\n",
      "Episode 4\tAverage Score -0.05\n",
      "Episode 4\tAverage Score -0.04\n",
      "Episode 4\tAverage Score -0.04\n",
      "Episode 4\tAverage Score -0.04\n",
      "Episode 4\tAverage Score -0.04\n",
      "Episode 4\tAverage Score -0.03\n",
      "Episode 4\tAverage Score -0.03\n",
      "Episode 4\tAverage Score -0.03\n",
      "Episode 4\tAverage Score -0.02\n",
      "Episode 4\tAverage Score -0.02\n",
      "Episode 4\tAverage Score -0.02\n",
      "Episode 4\tAverage Score -0.01\n",
      "Episode 4\tAverage Score -0.01\n",
      "Episode 4\tAverage Score -0.00\n",
      "Episode 4\tAverage Score 0.00\n",
      "Episode 4\tAverage Score 0.01\n",
      "Episode 4\tAverage Score 0.01\n",
      "Episode 4\tAverage Score 0.02\n",
      "Episode 4\tAverage Score 0.02\n",
      "Episode 4\tAverage Score 0.03\n",
      "Episode 4\tAverage Score 0.04\n",
      "Episode 4\tAverage Score 0.04\n",
      "Episode 4\tAverage Score 0.05\n",
      "Episode 4\tAverage Score 0.06\n",
      "Episode 4\tAverage Score 0.06\n",
      "Episode 4\tAverage Score 0.07\n",
      "Episode 4\tAverage Score 0.07\n",
      "Episode 4\tAverage Score 0.08\n",
      "Episode 4\tAverage Score 0.09\n",
      "Episode 4\tAverage Score 0.11\n",
      "--------\n",
      "Winner: 1\n",
      "\n",
      "=== EPISODE 5 ===\n",
      "Episode 5\tAverage Score 0.11\n",
      "Episode 5\tAverage Score 0.10\n",
      "Episode 5\tAverage Score 0.10\n",
      "Episode 5\tAverage Score 0.10\n",
      "Episode 5\tAverage Score 0.10\n",
      "Episode 5\tAverage Score 0.10\n",
      "Episode 5\tAverage Score 0.11\n",
      "Episode 5\tAverage Score 0.11\n",
      "Episode 5\tAverage Score 0.11\n",
      "Episode 5\tAverage Score 0.11\n",
      "Episode 5\tAverage Score 0.11\n",
      "Episode 5\tAverage Score 0.11\n",
      "Episode 5\tAverage Score 0.12\n",
      "Episode 5\tAverage Score 0.12\n",
      "Episode 5\tAverage Score 0.12\n",
      "Episode 5\tAverage Score 0.12\n",
      "Episode 5\tAverage Score 0.12\n",
      "Episode 5\tAverage Score 0.13\n",
      "Episode 5\tAverage Score 0.13\n",
      "Episode 5\tAverage Score 0.13\n",
      "Episode 5\tAverage Score 0.13\n",
      "Episode 5\tAverage Score 0.03\n",
      "--------\n",
      "Winner: 2\n",
      "\n",
      "=== EPISODE 6 ===\n",
      "Episode 6\tAverage Score 0.03\n",
      "Episode 6\tAverage Score 0.03\n",
      "Episode 6\tAverage Score 0.01\n",
      "Episode 6\tAverage Score 0.01\n",
      "Episode 6\tAverage Score 0.01\n",
      "Episode 6\tAverage Score 0.02\n",
      "Episode 6\tAverage Score 0.02\n",
      "Episode 6\tAverage Score 0.02\n",
      "Episode 6\tAverage Score 0.02\n",
      "Episode 6\tAverage Score 0.02\n",
      "Episode 6\tAverage Score 0.02\n",
      "Episode 6\tAverage Score 0.02\n",
      "Episode 6\tAverage Score 0.02\n",
      "Episode 6\tAverage Score 0.02\n",
      "Episode 6\tAverage Score 0.02\n",
      "Episode 6\tAverage Score 0.02\n",
      "Episode 6\tAverage Score 0.02\n",
      "Episode 6\tAverage Score 0.02\n",
      "Episode 6\tAverage Score 0.02\n",
      "Episode 6\tAverage Score 0.03\n",
      "Episode 6\tAverage Score 0.13\n",
      "Episode 6\tAverage Score 0.13\n",
      "Episode 6\tAverage Score 0.14\n",
      "Episode 6\tAverage Score 0.14\n",
      "Episode 6\tAverage Score 0.15\n",
      "Episode 6\tAverage Score 0.15\n",
      "Episode 6\tAverage Score 0.16\n",
      "Episode 6\tAverage Score 0.16\n",
      "Episode 6\tAverage Score 0.17\n",
      "Episode 6\tAverage Score 0.17\n",
      "Episode 6\tAverage Score 0.19\n",
      "--------\n",
      "Winner: 1\n",
      "\n",
      "=== EPISODE 7 ===\n",
      "Episode 7\tAverage Score 0.18\n",
      "Episode 7\tAverage Score 0.18\n",
      "Episode 7\tAverage Score 0.18\n",
      "Episode 7\tAverage Score 0.18\n",
      "Episode 7\tAverage Score 0.17\n",
      "Episode 7\tAverage Score 0.17\n",
      "Episode 7\tAverage Score 0.17\n",
      "Episode 7\tAverage Score 0.17\n",
      "Episode 7\tAverage Score 0.16\n",
      "Episode 7\tAverage Score 0.16\n",
      "Episode 7\tAverage Score 0.16\n",
      "Episode 7\tAverage Score 0.16\n",
      "Episode 7\tAverage Score 0.16\n",
      "Episode 7\tAverage Score 0.15\n",
      "Episode 7\tAverage Score 0.15\n",
      "Episode 7\tAverage Score 0.25\n",
      "Episode 7\tAverage Score 0.25\n",
      "Episode 7\tAverage Score 0.16\n",
      "--------\n",
      "Winner: 2\n",
      "\n",
      "=== EPISODE 8 ===\n",
      "Episode 8\tAverage Score 0.16\n",
      "Episode 8\tAverage Score 0.15\n",
      "Episode 8\tAverage Score 0.15\n",
      "Episode 8\tAverage Score 0.15\n",
      "Episode 8\tAverage Score 0.15\n",
      "Episode 8\tAverage Score 0.15\n",
      "Episode 8\tAverage Score 0.15\n",
      "Episode 8\tAverage Score 0.15\n",
      "Episode 8\tAverage Score 0.15\n",
      "Episode 8\tAverage Score 0.15\n",
      "Episode 8\tAverage Score 0.15\n",
      "Episode 8\tAverage Score 0.15\n",
      "Episode 8\tAverage Score 0.15\n",
      "Episode 8\tAverage Score 0.15\n",
      "Episode 8\tAverage Score 0.15\n",
      "Episode 8\tAverage Score 0.15\n",
      "Episode 8\tAverage Score 0.15\n",
      "Episode 8\tAverage Score 0.15\n",
      "Episode 8\tAverage Score 0.15\n",
      "Episode 8\tAverage Score 0.15\n",
      "Episode 8\tAverage Score 0.15\n",
      "Episode 8\tAverage Score 0.15\n",
      "Episode 8\tAverage Score 0.14\n",
      "Episode 8\tAverage Score 0.14\n",
      "Episode 8\tAverage Score 0.14\n",
      "Episode 8\tAverage Score 0.14\n",
      "Episode 8\tAverage Score 0.15\n",
      "--------\n",
      "Winner: 1\n",
      "\n",
      "=== EPISODE 9 ===\n",
      "Episode 9\tAverage Score 0.15\n",
      "Episode 9\tAverage Score 0.13\n",
      "Episode 9\tAverage Score 0.13\n",
      "Episode 9\tAverage Score 0.13\n",
      "Episode 9\tAverage Score 0.13\n",
      "Episode 9\tAverage Score 0.13\n",
      "Episode 9\tAverage Score 0.13\n",
      "Episode 9\tAverage Score 0.13\n",
      "Episode 9\tAverage Score 0.13\n",
      "Episode 9\tAverage Score 0.13\n",
      "Episode 9\tAverage Score 0.13\n",
      "Episode 9\tAverage Score 0.13\n",
      "Episode 9\tAverage Score 0.13\n",
      "Episode 9\tAverage Score 0.13\n",
      "Episode 9\tAverage Score 0.14\n",
      "Episode 9\tAverage Score 0.14\n",
      "Episode 9\tAverage Score 0.14\n",
      "Episode 9\tAverage Score 0.14\n",
      "Episode 9\tAverage Score 0.14\n",
      "Episode 9\tAverage Score 0.14\n",
      "Episode 9\tAverage Score 0.15\n",
      "--------\n",
      "Winner: 1\n",
      "\n",
      "=== EPISODE 10 ===\n",
      "Episode 10\tAverage Score 0.14\n",
      "Episode 10\tAverage Score 0.14\n",
      "Episode 10\tAverage Score 0.23\n",
      "Episode 10\tAverage Score 0.24\n",
      "Episode 10\tAverage Score 0.24\n",
      "Episode 10\tAverage Score 0.24\n",
      "Episode 10\tAverage Score 0.24\n",
      "Episode 10\tAverage Score 0.24\n",
      "Episode 10\tAverage Score 0.24\n",
      "Episode 10\tAverage Score 0.24\n",
      "Episode 10\tAverage Score 0.24\n",
      "Episode 10\tAverage Score 0.24\n",
      "Episode 10\tAverage Score 0.24\n",
      "Episode 10\tAverage Score 0.24\n",
      "Episode 10\tAverage Score 0.24\n",
      "Episode 10\tAverage Score 0.24\n",
      "Episode 10\tAverage Score 0.24\n",
      "Episode 10\tAverage Score 0.25\n",
      "Episode 10\tAverage Score 0.26\n",
      "--------\n",
      "Winner: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent1 = RandomPlayer() # Player1\r\n",
    "agent2 = RandomPlayer() # Player2\r\n",
    "\r\n",
    "def dqn(n_episodes= EPISODES, eps_start=EPS_START, eps_end = EPS_END, eps_decay=EPS_DECAY):\r\n",
    "    memory_buffer = ReplayMemory(BUFFER_SIZE)\r\n",
    "    \r\n",
    "    scores = []                         # list containing score from each episode\r\n",
    "    scores_window = deque(maxlen=100)   # last 100 scores\r\n",
    "    eps = eps_start\r\n",
    "\r\n",
    "    for i_episode in range(1, n_episodes+1):\r\n",
    "        print(f\"=== EPISODE {i_episode} ===\")\r\n",
    "        state = env.reset()\r\n",
    "        score = 0\r\n",
    "        \r\n",
    "        # # OPT A:\r\n",
    "        # one_game = env.play_one_game(agent1, agent2, each_step_render=False)\r\n",
    "        # for e in env.recording: print(e)\r\n",
    "\r\n",
    "        # OPT B:\r\n",
    "        players = [agent1, agent2]\r\n",
    "        env.observation_space = env.reset()\r\n",
    "        action = None\r\n",
    "        \r\n",
    "        while not env.game.game_state == gym_joinemio.envs.board.GameState.finished:\r\n",
    "            current_player = env.game.current_player - 1\r\n",
    "            action = players[current_player].get_action(env.observation_space)\r\n",
    "            state1 = env.game.board.grid\r\n",
    "            env.observation_space, reward, done, info = env.step(action)\r\n",
    "            state2 = env.game.board.grid\r\n",
    "\r\n",
    "            # Recording data:\r\n",
    "            if (current_player == 1): # (?) Do we do it for both players?\r\n",
    "                memory_buffer.push(state1, action, state2, reward.value) \r\n",
    "\r\n",
    "            score +=  reward.value\r\n",
    "            scores.append(score)\r\n",
    "            scores_window.append(score)\r\n",
    "            env.recording.append((current_player + 1, action, env.rewarder()))\r\n",
    "            \r\n",
    "            # Analyzing scores\r\n",
    "            eps = max(eps*eps_decay,eps_end)\r\n",
    "            print('Episode {}\\tAverage Score {:.2f}\\n'.format(i_episode,np.mean(scores_window)), end=\"\")\r\n",
    "            if np.mean(scores_window)>=200.0:\r\n",
    "                print('\\nEnvironment solve in {:d} epsiodes!\\tAverage score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\r\n",
    "                torch.save(agent.qnetwork_local.state_dict(),'checkpoint.pth')\r\n",
    "                break\r\n",
    "        \r\n",
    "        print(f\"--------\\nWinner: {env.game.winner}\\n\")\r\n",
    "        # return env.observation_space, reward, done, info  # reward for player1\r\n",
    "    return scores\r\n",
    "\r\n",
    "scores= dqn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6d05d1da31ed8d9449787bf5a0bbb8cb4d42b79e98256a8cd02d0c2de1610fa"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "8d71e197be79a8854e36fe80d7b81a808575f897a047ec71b5b71be66361356a"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}