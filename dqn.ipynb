{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import neptune\n",
    "import neptune.new as neptune\n",
    "import os\n",
    "\n",
    "token = os.getenv('NEPTUNE_API_TOKEN')\n",
    "proj = 'joinemio/test'\n",
    "run = neptune.init(project=proj,\n",
    "                   api_token=token)\n",
    "import time\n",
    "import gym\n",
    "import torch\n",
    "import random\n",
    "from collections import defaultdict, deque\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "from pprint import pprint\n",
    "import math\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from joblib import Parallel, delayed\n",
    "import gc\n",
    "import gym_joinemio\n",
    "from gym_joinemio.envs.player import RandomPlayer\n",
    "from gym_joinemio.envs.connect_four_env import Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neptune prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('joinemio-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size) # TODO: Batch size delete\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters: \n",
    "Random vs Random. Deep Q-Learning. Params:\n",
    "\n",
    "- `n_episodes` (int): maximum number of training epsiodes\n",
    "- `max_t` (int): maximum number of timesteps per episode _// Not used, because these episodes don't take too long and we like when game's are finished_\n",
    "- `eps_start` (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "- `eps_end` (float): minimum value of epsilon \n",
    "- `eps_decay` (float): mutiplicative factor (per episode) for decreasing epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\t\"EPISODES\" \t\t\t\t\t: 10,\n",
    "\t\"EPS_START\" \t\t\t\t: 1.0,\n",
    "\t\"EPS_END\" \t\t\t\t\t: 0.01,\n",
    "\t\"LEARNING_RATE\" \t\t\t: 1e-4,\n",
    "\t\"EPS_DECAY\" \t\t\t\t: 0.996,\n",
    "\t\"EPS_DECAY_LAST_FRAME\" \t\t: 10**5,\n",
    "\t\"BUFFER_SIZE\"\t\t\t\t: 300,\n",
    "\t\"FRAMES_MIN\" \t\t\t\t: 15000,\n",
    "\t\"MEAN_REWARD_BOUND\" \t\t: 0.80,\n",
    "\t\"SYNC_TARGET_FRAMES\" \t\t: 1000,\n",
    "\t\"REPLAY_START_SIZE\" \t\t: 10000\n",
    "}\n",
    "\n",
    "run[\"parameters\"] = params\n",
    "\n",
    "EPISODES = params[\"EPISODES\"]\n",
    "EPS_START = params[\"EPS_START\"]\n",
    "EPS_END = params[\"EPS_END\"]\n",
    "LEARNING_RATE = params[\"LEARNING_RATE\"]\n",
    "EPS_DECAY = params[\"EPS_DECAY\"]\n",
    "EPS_DECAY_LAST_FRAME = params[\"EPS_DECAY_LAST_FRAME\"] # 10**5\n",
    "\n",
    "BUFFER_SIZE = params[\"BUFFER_SIZE\"]\n",
    "FRAMES_MIN = params[\"FRAMES_MIN\"]\n",
    "MEAN_REWARD_BOUND = params[\"MEAN_REWARD_BOUND\"]\n",
    "N_FOR_MEAN_REWARD = BUFFER_SIZE\n",
    "\n",
    "SYNC_TARGET_FRAMES = params[\"SYNC_TARGET_FRAMES\"]\n",
    "REPLAY_START_SIZE = params[\"REPLAY_START_SIZE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_joinemio.envs.board import Board, GameState\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.main_layers = nn.Sequential(\n",
    "            nn.Linear(6*7, 6*7),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6*7, 6*7),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6*7, 6*7),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.output_layer = nn.Linear(6*7,7)\n",
    "\n",
    "    def forward(self, board_flatten_state):\n",
    "        weights = self.output_layer(self.main_layers(board_flatten_state)) # weights = nn.relU(self.output_layer(self.main_layers(board_flatten_state)))\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIPlayer:\n",
    "    @staticmethod\n",
    "    def possible_moves(board_state):\n",
    "        available_cols = []\n",
    "        for i in range(len(board_state[0])):\n",
    "            if board_state[0][i] == 0:\n",
    "                available_cols.append(i)\n",
    "        return available_cols\n",
    "\n",
    "    def __init__(self, env, replay_memory, net):\n",
    "        self.net = net\n",
    "        self.env = env\n",
    "        self.env.opponent_action_set(RandomPlayer.get_action)\n",
    "        self.replay_memory = replay_memory\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.env = gym.make('joinemio-v0')\n",
    "        self.state =  self.env.reset()\n",
    "        self.env.opponent_action_set(RandomPlayer.get_action)#self.get_action)\n",
    "        self.env.game = gym_joinemio.envs.board.Game()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def get_action(self, board_state):\n",
    "        weigths = self.net.forward(torch.flatten(torch.FloatTensor(board_state.astype(float)).type(torch.FloatTensor)))\n",
    "        pos_nums = self.possible_moves(board_state)\n",
    "        max_num = 0\n",
    "        for col in pos_nums:\n",
    "            if weigths[max_num] < weigths[int(col)]:\n",
    "                max_num = int(col)\n",
    "        return max_num\n",
    "\n",
    "    def get_net_action(self, grid):\n",
    "        state_a = np.array(self.state, copy=False, dtype=np.uint8)\n",
    "        state_v = torch.flatten(torch.FloatTensor(state_a).to(device))\n",
    "        q_vals_v = self.net(state_v)\n",
    "        # TODO filter\n",
    "        for i in range(Board.columns):\n",
    "            if i not in self.possible_moves(grid):\n",
    "                q_vals_v[i] = -100\n",
    "        _, act_v = torch.max(q_vals_v, dim=0)\n",
    "        action = int(act_v.item())\n",
    "        return action\n",
    "\n",
    "    def play_step(self, net, epsilon, device):\n",
    "        done_reward = None\n",
    "        final_reward = None\n",
    "        ## print(\"*AI PLAYER PLAYED*\")\n",
    "        state_a = np.array(self.state, copy=False, dtype=np.uint8)\n",
    "        state_v = torch.flatten(torch.FloatTensor(state_a).to(device))\n",
    "        q_vals_v = net(state_v)\n",
    "\n",
    "        grid = self.env.game.board.grid\n",
    "        for i in range(Board.columns):\n",
    "            if i not in self.possible_moves(grid):\n",
    "                q_vals_v[i] = -1\n",
    "        _, act_v = torch.max(q_vals_v, dim=0)\n",
    "        action = int(act_v.item())  # TODO: Later check if it returns correct action int range.\n",
    "\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        final_reward = reward\n",
    "        self.total_reward += reward.value\n",
    "\n",
    "        self.replay_memory.push(self.state, action, new_state, reward)\n",
    "        self.state = new_state\n",
    "\n",
    "        if is_done == GameState.finished:\n",
    "            done_reward = self.total_reward\n",
    "            ## print(f\"WINNER: player {self.env.game.winner} | Final reward reading: {final_reward}\")\n",
    "            ## print(self.env.game.board.grid)\n",
    "            self._reset()\n",
    "            return (done_reward, final_reward)\n",
    "        else: return (done_reward, None)\n",
    "\n",
    "    def train(self, memory_buffer, batch_size):\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_joinemio.envs.game_window import GameWindow\n",
    "from pyglet import clock\n",
    "from pyglet import app\n",
    "\n",
    "def play_with_ai(agent):\n",
    "    window = GameWindow(agent) #(agent.get_net_action, agent.env.game)\n",
    "    clock.schedule_interval(window.update, 1000) # GameWindow.refresh_rate)\n",
    "    app.run()\n",
    "\n",
    "def our_main():\n",
    "    env = gym.make('joinemio-v0')\n",
    "    net = NeuralNetwork().to(device)\n",
    "    tgt_net = NeuralNetwork().to(device)\n",
    "    print(net)\n",
    "    buffer = ReplayMemory(BUFFER_SIZE)\n",
    "    global agent \n",
    "    agent = AIPlayer(env, buffer, net) # TODO: params? Whatever agent is...\n",
    "    epsilon = EPS_START\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    total_rewards = []\n",
    "    frame_idx = 0\n",
    "    ts_frame = 0\n",
    "    ts = time.time()\n",
    "    best_mean_reward = None\n",
    "    last_total_frames = 0\n",
    "    while True:\n",
    "        frame_idx += 1\n",
    "        epsilon = max(EPS_END, EPS_START - frame_idx / EPS_DECAY_LAST_FRAME)\n",
    "        reward, final_reward = agent.play_step(net, epsilon, device=device)\n",
    "        if (final_reward is not None):\n",
    "            frames_this_game = frame_idx - last_total_frames\n",
    "            last_total_frames = frame_idx\n",
    "            ## print(\"Reward is not None (Game Finished)\")\n",
    "            total_rewards.append(reward)\n",
    "            ts_frame = frame_idx\n",
    "            ts = time.time()\n",
    "            mean_reward = np.mean(total_rewards[-N_FOR_MEAN_REWARD:])\n",
    "            ## print(f'Total Steps Played: {frame_idx}, steps this game {frames_this_game}, Games done: {len(total_rewards)}, mean reward: {mean_reward}, eps: {epsilon}, final reward: {final_reward}')\n",
    "            ## print(\"\")          # TODO: # Neptun logging (write epsilon, speed, reward_100, reward)\n",
    "            run[\"eps\"].log(epsilon)\n",
    "            run[\"games_done\"].log(len(total_rewards))\n",
    "            run[\"mean_reward\"].log(mean_reward)\n",
    "            \n",
    "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "                torch.save(net.state_dict(), \"joinemio-best.dat\") # TODO: Extract\n",
    "                if best_mean_reward is not None:\n",
    "                    print(f\"Best mean reward updated {best_mean_reward}->{mean_reward}; model saved\")\n",
    "                best_mean_reward = mean_reward\n",
    "\n",
    "            condition = mean_reward > MEAN_REWARD_BOUND and frame_idx > FRAMES_MIN\n",
    "            if condition: \n",
    "                ## print(f\"Solved in {frame_idx} frames\")\n",
    "                break\n",
    "        else: pass ## print(\"REWARD NONE\")\n",
    "        \n",
    "        if len(buffer) < REPLAY_START_SIZE:\n",
    "            continue\n",
    "\n",
    "        if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
    "            tgt_net.load_state_dict(net.state_dict())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch = buffer.sample(BATCH_SIZE)\n",
    "        # loss_t = calc_loss(batch, net, tgt_net, device=device) # TODO: calc_loss method\n",
    "        optimizer.step()\n",
    "\n",
    "    # play_with_ai(agent)\n",
    "\n",
    "our_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_with_ai(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d71e197be79a8854e36fe80d7b81a808575f897a047ec71b5b71be66361356a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "8d71e197be79a8854e36fe80d7b81a808575f897a047ec71b5b71be66361356a"
   }
  },
  "neptune": {
   "notebookId": "08ae55f4-ed57-4b07-9d50-74efafc39e30",
   "projectVersion": 2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
