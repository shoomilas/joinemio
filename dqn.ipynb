{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **JOINEMIO**\r\n",
    "## A project that uses AI to play Connect4\r\n",
    "Project created by three AGH students:\r\n",
    "* <a href=\"https://github.com/shoomilas\" target=\"_blank\">Jakub Szumilas</a>\r\n",
    "* <a href=\"https://github.com/KarolSzeliga4\" target=\"_blank\">Karol Szeliga</a>\r\n",
    "* <a href=\"https://github.com/gzelek8\" target=\"_blank\">Grzegorz Zelek</a>\r\n",
    "\r\n",
    "Final project for the `PitE-Summer-2021` subject. Its main goal was to train the AI ​​player in such a way that he could smoothly play the game against a player making random moves. \r\n",
    "The implementation was divided into several stages: \r\n",
    "* 1.The first was to create an imitation of a game with a graphical interface where it was possible to play two players. \r\n",
    "* 2.The next step was to create the game environment needed to implement the next steps. An open ai gym environment was used for this traffic jam. \r\n",
    "* 3.Then, an appropriate method and an appropriate algorithm were selected to solve the problem. The choice fell on alfoytm deep q learning and the use of the Multi-armed bandit alogytm. \r\n",
    "* 4.The last step was to select the appropriate parameters and summarize the results. \r\n",
    "\r\n",
    "The project achieved its intended goal and after training most of the games, it beats the opponent making random moves."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initial imports\r\n",
    "### The libraries used are:\r\n",
    "* `pytorch` (assistance in the implementation of the neural network and the network learning process)\r\n",
    "* `numpy` (assistance with operations related to the board)\r\n",
    "* `gym` (assistance with the implementation of the environment)\r\n",
    "* `neptune` (assistance in monitoring the learning process)\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import neptune\r\n",
    "import neptune.new as neptune\r\n",
    "import os\r\n",
    "\r\n",
    "token = os.getenv('NEPTUNE_API_TOKEN')\r\n",
    "proj = 'joinemio/test'\r\n",
    "run = neptune.init(project=proj,\r\n",
    "                   api_token=token)\r\n",
    "import time\r\n",
    "import gym\r\n",
    "import torch\r\n",
    "import random\r\n",
    "from collections import defaultdict, deque\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import torch.nn.functional as F\r\n",
    "from collections import namedtuple, deque\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import copy\r\n",
    "from pprint import pprint\r\n",
    "import math\r\n",
    "from tqdm.autonotebook import tqdm\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from torchvision import transforms\r\n",
    "from joblib import Parallel, delayed\r\n",
    "import gc\r\n",
    "import gym_joinemio\r\n",
    "from gym_joinemio.envs.player import RandomPlayer\r\n",
    "from gym_joinemio.envs.connect_four_env import Reward"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We made an environment implanted by ourselves, which helped us understand the entire preparation process."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env = gym.make('joinemio-v0')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to speed up the learning process, we used calculations with the use of a graphics card"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "print(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Replay memory\r\n",
    "All of the agent's experiences at each time step over all episodes played by the agent are stored in the replay memory. Well actually, in practice, we'll usually see the replay memory set to some finite size limit, , and therefore, it will only store the last  experiences.\r\n",
    "\r\n",
    "This replay memory data set is what we'll randomly sample from to train the network. The act of gaining experience and sampling from the replay memory that stores these experience is called experience replay.\r\n",
    "\r\n",
    "A key reason for using replay memory is to break the correlation between consecutive samples.\r\n",
    "If the network learned only from consecutive samples of experience as they occurred sequentially in the environment, the samples would be highly correlated and would therefore lead to inefficient learning. Taking random samples from replay memory breaks this correlation.\r\n",
    "***\r\n",
    "### Replay memory steps:\r\n",
    " - Initialize replay memory capacity.\r\n",
    " - Initialize the network with random weights.\r\n",
    " - For each episode:\r\n",
    " \r\n",
    "   - Initialize the starting state.\r\n",
    "\r\n",
    "   - For each time step:\r\n",
    "\r\n",
    "      - Select an action.\r\n",
    "\r\n",
    "        - Via exploration or exploitation\r\n",
    "\r\n",
    "      - Execute selected action in an emulator.\r\n",
    "\r\n",
    "      - Observe reward and next state.\r\n",
    "            \r\n",
    "      - Store experience in replay memory.\r\n",
    "***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Transition = namedtuple('Transition',\r\n",
    "                        ('state', 'action', 'next_state', 'reward'))\r\n",
    "\r\n",
    "class ReplayMemory(object):\r\n",
    "    def __init__(self, capacity):\r\n",
    "        self.memory = deque([],maxlen=capacity)\r\n",
    "\r\n",
    "    def push(self, *args):\r\n",
    "        \"\"\"Save a transition\"\"\"\r\n",
    "        self.memory.append(Transition(*args))\r\n",
    "\r\n",
    "    def sample(self, batch_size):\r\n",
    "        return random.sample(self.memory, batch_size) # TODO: Batch size delete\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.memory)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training parameters: \n",
    "Random vs Random. Deep Q-Learning. Params:\n",
    "\n",
    "- `n_episodes` (int): maximum number of training epsiodes\n",
    "- `max_t` (int): maximum number of timesteps per episode _// Not used, because these episodes don't take too long and we like when game's are finished_\n",
    "- `eps_start` (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "- `eps_end` (float): minimum value of epsilon \n",
    "- `eps_decay` (float): mutiplicative factor (per episode) for decreasing epsilon"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params = {\r\n",
    "\t\"EPISODES\" \t\t\t\t\t: 10,\r\n",
    "\t\"EPS_START\" \t\t\t\t: 1.0,\r\n",
    "\t\"EPS_END\" \t\t\t\t\t: 0.01,\r\n",
    "\t\"LEARNING_RATE\" \t\t\t: 1e-4,\r\n",
    "\t\"EPS_DECAY\" \t\t\t\t: 0.996,\r\n",
    "\t\"EPS_DECAY_LAST_FRAME\" \t\t: 10**5,\r\n",
    "\t\"BUFFER_SIZE\"\t\t\t\t: 300,\r\n",
    "\t\"FRAMES_MIN\" \t\t\t\t: 15000,\r\n",
    "\t\"MEAN_REWARD_BOUND\" \t\t: 0.80,\r\n",
    "\t\"SYNC_TARGET_FRAMES\" \t\t: 1000,\r\n",
    "\t\"REPLAY_START_SIZE\" \t\t: 10000\r\n",
    "}\r\n",
    "\r\n",
    "run[\"parameters\"] = params\r\n",
    "\r\n",
    "EPISODES = params[\"EPISODES\"]\r\n",
    "EPS_START = params[\"EPS_START\"]\r\n",
    "EPS_END = params[\"EPS_END\"]\r\n",
    "LEARNING_RATE = params[\"LEARNING_RATE\"]\r\n",
    "EPS_DECAY = params[\"EPS_DECAY\"]\r\n",
    "EPS_DECAY_LAST_FRAME = params[\"EPS_DECAY_LAST_FRAME\"] # 10**5\r\n",
    "\r\n",
    "BUFFER_SIZE = params[\"BUFFER_SIZE\"]\r\n",
    "FRAMES_MIN = params[\"FRAMES_MIN\"]\r\n",
    "MEAN_REWARD_BOUND = params[\"MEAN_REWARD_BOUND\"]\r\n",
    "N_FOR_MEAN_REWARD = BUFFER_SIZE\r\n",
    "\r\n",
    "SYNC_TARGET_FRAMES = params[\"SYNC_TARGET_FRAMES\"]\r\n",
    "REPLAY_START_SIZE = params[\"REPLAY_START_SIZE\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NeuralNetwork\r\n",
    "For the learning process, we used a neural network consisting of 4 layers, 42 input neurons on each of them and  7 neurons, each corresponding to the action of dropping a chip into one of 7 different columns of the board. At the input, we place a flattened board with 42 elements depicting the game boards, at the output the profitability weight of the token toss on a given selected column\r\n",
    "\r\n",
    "In a neural network, the activation function is responsible for transforming the summed weighted input from the node into the activation of the node or output for that input. The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance.\r\n",
    "\r\n",
    "We used `relu` functions as activation functions on our layers\r\n",
    "This function returns 0 if it receives any negative input, but for any positive value  x  it returns that value back. So it can be written as  f(x)=max(0,x)\r\n",
    "\r\n",
    "***\r\n",
    "*Neural Network Architecture Diagram.*\r\n",
    "\r\n",
    "![Neural Network Architecture Scheme.](https://raw.githubusercontent.com/shoomilas/joinemio/main/docs/img/NeurNetArch.png)\r\n",
    "***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gym_joinemio.envs.board import Board, GameState\r\n",
    "\r\n",
    "\r\n",
    "class NeuralNetwork(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(NeuralNetwork, self).__init__()\r\n",
    "\r\n",
    "        self.main_layers = nn.Sequential(\r\n",
    "            nn.Linear(6*7, 6*7),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(6*7, 6*7),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(6*7, 6*7),\r\n",
    "            nn.ReLU()\r\n",
    "        )\r\n",
    "        self.output_layer = nn.Linear(6*7,7)\r\n",
    "\r\n",
    "    def forward(self, board_flatten_state):\r\n",
    "        weights = self.output_layer(self.main_layers(board_flatten_state))\r\n",
    "        return weights"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AIPlayer\r\n",
    "\r\n",
    "\r\n",
    "The learning process was carried out using deep q learning. ore specifically, the agents receives information on the current observation (the current state of the board) and then has to take an action (which slot to choose to add a coin). After that, nature responses with a new state and potentially yields a reward (if the game is won) or a penalty (if the game is lost or if the agent chooses an action that is not valid - such as putting a coin into an already full slot). The goal of each action is to receive the greatest possible reward. After gathering some experience, a neural network is trained to make sense of the state, action and reward relationship. The target is set such that the network aims at minimizing the loss between predicting the reward of the next_state and the realized reward.\r\n",
    "\r\n",
    "In all `Reinforcement Learning` problems, there exists an unavoidable trade-off between exploration and exploitation. Exploration is where an agent makes some out of character decision (not dictated by the network) to try to find new strategies and potentially be rewarded from them; thus, an opportunity to learn better strategies. Exploitation is to exploit the strategies the network has already learned. For example, if an agent has learned a bit how to play Connect 4 and it always exploits the techniques it has already learned, it will never be presented with new observation/action pairs to learn from. On the other hand, if the agent is always exploring new strategies, it is essentially always playing random moves and never reaching a high level of game play to learn from. The way this problem is dealt with typically is through a phenomenon known as epsilon decay. The epsilon in epsilon decay can be thought of as the probability the agent will choose a random action rather than an action as chosen by then network. The decay in epsilon decay is just that: since towards the beginning of the training cycle we expect the agent to be dumb, for lack of a better term, we will let it make largely random decisions, thus letting the agent explore more than exploit, since it hasn’t yet learned anything to exploit. Over time, the epsilon will get smaller and smaller, that is: the agent will start exploiting its learned knowledge more and more; and will explore random actions less and less. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class AIPlayer:\r\n",
    "    @staticmethod\r\n",
    "    def possible_moves(board_state):\r\n",
    "        available_cols = []\r\n",
    "        for i in range(len(board_state[0])):\r\n",
    "            if board_state[0][i] == 0:\r\n",
    "                available_cols.append(i)\r\n",
    "        return available_cols\r\n",
    "\r\n",
    "    def __init__(self, env, replay_memory, net):\r\n",
    "        self.net = net\r\n",
    "        self.env = env\r\n",
    "        self.env.opponent_action_set(RandomPlayer.get_action)\r\n",
    "        self.replay_memory = replay_memory\r\n",
    "        self._reset()\r\n",
    "\r\n",
    "    def _reset(self):\r\n",
    "        self.env = gym.make('joinemio-v0')\r\n",
    "        self.state =  self.env.reset()\r\n",
    "        self.env.opponent_action_set(RandomPlayer.get_action)\r\n",
    "        self.env.game = gym_joinemio.envs.board.Game()\r\n",
    "        self.total_reward = 0.0\r\n",
    "\r\n",
    "    def get_action(self, board_state):\r\n",
    "        weigths = self.net.forward(torch.flatten(torch.FloatTensor(board_state.astype(float)).type(torch.FloatTensor)))\r\n",
    "        pos_nums = self.possible_moves(board_state)\r\n",
    "        max_num = 0\r\n",
    "        for col in pos_nums:\r\n",
    "            if weigths[max_num] < weigths[int(col)]:\r\n",
    "                max_num = int(col)\r\n",
    "        return max_num\r\n",
    "\r\n",
    "    def get_net_action(self, grid):\r\n",
    "        state_a = np.array(self.state, copy=False, dtype=np.uint8)\r\n",
    "        state_v = torch.flatten(torch.FloatTensor(state_a).to(device))\r\n",
    "        q_vals_v = self.net(state_v)\r\n",
    "        # TODO filter\r\n",
    "        for i in range(Board.columns):\r\n",
    "            if i not in self.possible_moves(grid):\r\n",
    "                q_vals_v[i] = -100\r\n",
    "        _, act_v = torch.max(q_vals_v, dim=0)\r\n",
    "        action = int(act_v.item())\r\n",
    "        return action\r\n",
    "\r\n",
    "    def play_step(self, net, epsilon, device):\r\n",
    "        done_reward = None\r\n",
    "        final_reward = None\r\n",
    "        ## print(\"*AI PLAYER PLAYED*\")\r\n",
    "        state_a = np.array(self.state, copy=False, dtype=np.uint8)\r\n",
    "        state_v = torch.flatten(torch.FloatTensor(state_a).to(device))\r\n",
    "        q_vals_v = net(state_v)\r\n",
    "\r\n",
    "        grid = self.env.game.board.grid\r\n",
    "        for i in range(Board.columns):\r\n",
    "            if i not in self.possible_moves(grid):\r\n",
    "                q_vals_v[i] = -1\r\n",
    "        _, act_v = torch.max(q_vals_v, dim=0)\r\n",
    "        action = int(act_v.item())  \r\n",
    "\r\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\r\n",
    "        final_reward = reward\r\n",
    "        self.total_reward += reward.value\r\n",
    "\r\n",
    "        self.replay_memory.push(self.state, action, new_state, reward)\r\n",
    "        self.state = new_state\r\n",
    "\r\n",
    "        if is_done == GameState.finished:\r\n",
    "            done_reward = self.total_reward\r\n",
    "            self._reset()\r\n",
    "            return (done_reward, final_reward)\r\n",
    "        else: return (done_reward, None)\r\n",
    "\r\n",
    "    def train(self, memory_buffer, batch_size):\r\n",
    "        return 0\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training loop:\r\n",
    "Training is nothing as iteratively playing against the trainer, memorizing what happened and updating the neural net weights after each iteration.\r\n",
    "\r\n",
    "So it looks like `Deep-Q-Learning` was the right choice: just by playing against a random agent, the neural network was trained to win the game - even without knowing the rules first!\r\n",
    "\r\n",
    "***\r\n",
    "*Deep Q Learning - An example of a path to a game-ending state in a game subtree*\r\n",
    "\r\n",
    "![DEEP Q.](https://miro.medium.com/max/700/1*NKzsRiAxa_oiikgbLyLCyw.png)\r\n",
    "***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gym_joinemio.envs.game_window import GameWindow\r\n",
    "from pyglet import clock\r\n",
    "from pyglet import app\r\n",
    "\r\n",
    "def play_with_ai(agent):\r\n",
    "    window = GameWindow(agent)\r\n",
    "    clock.schedule_interval(window.update, 1000) \r\n",
    "    app.run()\r\n",
    "\r\n",
    "def our_main():\r\n",
    "    env = gym.make('joinemio-v0')\r\n",
    "    net = NeuralNetwork().to(device)\r\n",
    "    tgt_net = NeuralNetwork().to(device)\r\n",
    "    print(net)\r\n",
    "    buffer = ReplayMemory(BUFFER_SIZE)\r\n",
    "    global agent \r\n",
    "    agent = AIPlayer(env, buffer, net)\r\n",
    "    epsilon = EPS_START\r\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\r\n",
    "    \r\n",
    "    total_rewards = []\r\n",
    "    frame_idx = 0\r\n",
    "    ts_frame = 0\r\n",
    "    ts = time.time()\r\n",
    "    best_mean_reward = None\r\n",
    "    last_total_frames = 0\r\n",
    "    while True:\r\n",
    "        frame_idx += 1\r\n",
    "        epsilon = max(EPS_END, EPS_START - frame_idx / EPS_DECAY_LAST_FRAME)\r\n",
    "        reward, final_reward = agent.play_step(net, epsilon, device=device)\r\n",
    "        if (final_reward is not None):\r\n",
    "            frames_this_game = frame_idx - last_total_frames\r\n",
    "            last_total_frames = frame_idx\r\n",
    "            total_rewards.append(reward)\r\n",
    "            ts_frame = frame_idx\r\n",
    "            ts = time.time()\r\n",
    "            mean_reward = np.mean(total_rewards[-N_FOR_MEAN_REWARD:])\r\n",
    "            run[\"eps\"].log(epsilon)\r\n",
    "            run[\"games_done\"].log(len(total_rewards))\r\n",
    "            run[\"mean_reward\"].log(mean_reward)\r\n",
    "            \r\n",
    "            if best_mean_reward is None or best_mean_reward < mean_reward:\r\n",
    "                torch.save(net.state_dict(), \"joinemio-best.dat\") # TODO: Extract\r\n",
    "                if best_mean_reward is not None:\r\n",
    "                    print(f\"Best mean reward updated {best_mean_reward}->{mean_reward}; model saved\")\r\n",
    "                best_mean_reward = mean_reward\r\n",
    "\r\n",
    "            condition = mean_reward > MEAN_REWARD_BOUND and frame_idx > FRAMES_MIN\r\n",
    "            if condition:\r\n",
    "                break\r\n",
    "        else: pass \r\n",
    "        \r\n",
    "        if len(buffer) < REPLAY_START_SIZE:\r\n",
    "            continue\r\n",
    "\r\n",
    "        if frame_idx % SYNC_TARGET_FRAMES == 0:\r\n",
    "            tgt_net.load_state_dict(net.state_dict())\r\n",
    "\r\n",
    "        optimizer.zero_grad()\r\n",
    "        batch = buffer.sample(BATCH_SIZE)\r\n",
    "        optimizer.step()\r\n",
    "\r\n",
    "our_main()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "run.stop()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "play_with_ai(agent)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusions and observations\r\n",
    "We have brought the network to the stage where the network learns when playing with a player making random moves. We had a problem with looping the learning process, so we used multi-armed bandit algorithm. It allowed us to break these loops. Initially, the learning player performs random moves in the same way as the opponent, but with subsequent moves, moves selected by the network are interfered more and more often. The parameter ε is responsible for the frequency of these movements. An increasingly well-trained network makes more and more non-random moves, which leads to a much larger number of won games. The network discovered that when playing with a player making random moves, the best chance of winning is by placing tokens in one column all the time.\r\n",
    "\r\n",
    "Deep Q-Learning can handle any observation space so long as the observations themselves are not too data intensive for the network. I feel this sort of algorithm is more useful to experiment with as it is more likely to be implemented in real world environments"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d71e197be79a8854e36fe80d7b81a808575f897a047ec71b5b71be66361356a"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "8d71e197be79a8854e36fe80d7b81a808575f897a047ec71b5b71be66361356a"
   }
  },
  "neptune": {
   "notebookId": "08ae55f4-ed57-4b07-9d50-74efafc39e30",
   "projectVersion": 2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}