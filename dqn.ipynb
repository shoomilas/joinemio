{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Initial imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\r\n",
    "import time\r\n",
    "import gym\r\n",
    "import torch\r\n",
    "import random\r\n",
    "from collections import defaultdict, deque\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import torch.nn.functional as F\r\n",
    "from collections import namedtuple, deque\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import copy\r\n",
    "from pprint import pprint\r\n",
    "import math\r\n",
    "from tqdm.autonotebook import tqdm\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from torchvision import transforms\r\n",
    "from joblib import Parallel, delayed\r\n",
    "import gc\r\n",
    "import gym_joinemio\r\n",
    "from gym_joinemio.envs.player import RandomPlayer\r\n",
    "from gym_joinemio.envs.connect_four_env import Reward"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Neptune prep"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# import neptune\r\n",
    "# import neptune.new as neptune\r\n",
    "# import os\r\n",
    "\r\n",
    "# proj = 'jmolais/joinemio'\r\n",
    "# token = os.getenv('JOINEMIO_TOKEN')\r\n",
    "# run = neptune.init(project=proj,\r\n",
    "#                    api_token=token)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "env = gym.make('joinemio-v0')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "# device = torch.device(\"cpu\")\r\n",
    "print(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Replay memory"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "Transition = namedtuple('Transition',\r\n",
    "                        ('state', 'action', 'next_state', 'reward'))\r\n",
    "\r\n",
    "class ReplayMemory(object):\r\n",
    "    def __init__(self, capacity):\r\n",
    "        self.memory = deque([],maxlen=capacity)\r\n",
    "\r\n",
    "    def push(self, *args):\r\n",
    "        \"\"\"Save a transition\"\"\"\r\n",
    "        self.memory.append(Transition(*args))\r\n",
    "\r\n",
    "    def sample(self, batch_size):\r\n",
    "        return random.sample(self.memory, batch_size) # TODO: Batch size delete\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.memory)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# agent1 = RandomPlayer() # Player1\r\n",
    "# agent2 = RandomPlayer() # Player2\r\n",
    "\r\n",
    "# def dqn(n_episodes= EPISODES, eps_start=EPS_START, eps_end = EPS_END, eps_decay=EPS_DECAY):\r\n",
    "#     memory_buffer = ReplayMemory(BUFFER_SIZE)\r\n",
    "    \r\n",
    "#     scores = []                         # list containing score from each episode\r\n",
    "#     scores_window = deque(maxlen=100)   # last 100 scores\r\n",
    "#     eps = eps_start\r\n",
    "\r\n",
    "#     for i_episode in range(1, n_episodes+1):\r\n",
    "#         print(f\"=== EPISODE {i_episode} ===\")\r\n",
    "#         state = env.reset()\r\n",
    "#         score = 0\r\n",
    "        \r\n",
    "#         # # OPT A:\r\n",
    "#         # one_game = env.play_one_game(agent1, agent2, each_step_render=False)\r\n",
    "#         # for e in env.recording: print(e)\r\n",
    "\r\n",
    "#         # OPT B:\r\n",
    "#         players = [agent1, agent2]\r\n",
    "#         env.observation_space = env.reset()\r\n",
    "#         action = None\r\n",
    "        \r\n",
    "#         while not env.game.game_state == gym_joinemio.envs.board.GameState.finished:\r\n",
    "#             current_player = env.game.current_player - 1\r\n",
    "#             action = players[current_player].get_action(env.observation_space)\r\n",
    "#             state1 = env.game.board.grid\r\n",
    "#             env.observation_space, reward, done, info = env.step(action)\r\n",
    "#             state2 = env.game.board.grid\r\n",
    "\r\n",
    "#             # Recording data:\r\n",
    "#             if (current_player == 1): # (?) Do we do it for both players?\r\n",
    "#                 memory_buffer.push(state1, action, state2, reward.value) \r\n",
    "\r\n",
    "#             score +=  reward.value\r\n",
    "#             scores.append(score)\r\n",
    "#             scores_window.append(score)\r\n",
    "#             env.recording.append((current_player + 1, action, env.rewarder()))\r\n",
    "            \r\n",
    "#             # Analyzing scores\r\n",
    "#             eps = max(eps*eps_decay,eps_end)\r\n",
    "#             print('Episode {}\\tAverage Score {:.2f}\\n'.format(i_episode,np.mean(scores_window)), end=\"\")\r\n",
    "#             if np.mean(scores_window)>=200.0:\r\n",
    "#                 print('\\nEnvironment solve in {:d} epsiodes!\\tAverage score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\r\n",
    "#                 torch.save(agent.qnetwork_local.state_dict(),'checkpoint.pth')\r\n",
    "#                 break\r\n",
    "        \r\n",
    "#         print(f\"--------\\nWinner: {env.game.winner}\\n\")\r\n",
    "#         # return env.observation_space, reward, done, info  # reward for player1\r\n",
    "#     return scores\r\n",
    "\r\n",
    "# scores = dqn()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training parameters: \r\n",
    "Random vs Random. Deep Q-Learning. Params:\r\n",
    "\r\n",
    "- `n_episodes` (int): maximum number of training epsiodes\r\n",
    "- `max_t` (int): maximum number of timesteps per episode _// Not used, because these episodes don't take too long and we like when game's are finished_\r\n",
    "- `eps_start` (float): starting value of epsilon, for epsilon-greedy action selection\r\n",
    "- `eps_end` (float): minimum value of epsilon \r\n",
    "- `eps_decay` (float): mutiplicative factor (per episode) for decreasing epsilon"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "BUFFER_SIZE = 200\r\n",
    "EPISODES = 10\r\n",
    "EPS_START = 1.0\r\n",
    "EPS_END = 0.01\r\n",
    "EPS_DECAY = 0.996\r\n",
    "EPS_DECAY_LAST_FRAME = 10**5\r\n",
    "LEARNING_RATE = 1e-4\r\n",
    "\r\n",
    "MEAN_REWARD_BOUND = 0.8 # TODO evaluate proper default val\r\n",
    "\r\n",
    "\r\n",
    "# GAMMA = 0.99\r\n",
    "# BATCH_SIZE = 32\r\n",
    "# REPLAY_SIZE = 10000\r\n",
    "# LEARNING_RATE = 1e-4\r\n",
    "SYNC_TARGET_FRAMES = 1000\r\n",
    "REPLAY_START_SIZE = 10000\r\n",
    "\r\n",
    "# EPSILON_DECAY_LAST_FRAME = 10**5\r\n",
    "# EPSILON_START = 1.0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NeuralNetwork"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from gym_joinemio.envs.board import Board, GameState\r\n",
    "\r\n",
    "\r\n",
    "class NeuralNetwork(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(NeuralNetwork, self).__init__()\r\n",
    "\r\n",
    "        self.main_layers = nn.Sequential(\r\n",
    "            nn.Linear(6*7, 6*7),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(6*7, 6*7),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(6*7, 6*7),\r\n",
    "            nn.ReLU()\r\n",
    "        )\r\n",
    "        self.output_layer = nn.Linear(6*7,7)\r\n",
    "\r\n",
    "    def forward(self, board_flatten_state):\r\n",
    "        # x1 = torch.tensor(input.astype(float)).type(torch.FloatTensor)\r\n",
    "        # x = torch.flatten(x1)\r\n",
    "        # for layer in self.net:\r\n",
    "        #     x = layer(x)\r\n",
    "        weights = self.output_layer(self.main_layers(board_flatten_state)) # weights = nn.relU(self.output_layer(self.main_layers(board_flatten_state)))\r\n",
    "        return weights"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AIPlayer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class AIPlayer:\r\n",
    "    @staticmethod\r\n",
    "    def possible_moves(board_state):\r\n",
    "        available_cols = []\r\n",
    "        for i in range(len(board_state[0])):\r\n",
    "            if board_state[0][i] == 0:\r\n",
    "                available_cols.append(i)\r\n",
    "        return available_cols\r\n",
    "\r\n",
    "    def __init__(self, env, replay_memory):\r\n",
    "        # self.net = NeuralNetwork()\r\n",
    "        self.env = env\r\n",
    "        self.env.opponent_action_set(RandomPlayer.get_action)#self.get_action)\r\n",
    "        self.replay_memory = replay_memory\r\n",
    "        self._reset()\r\n",
    "\r\n",
    "    def _reset(self):\r\n",
    "        self.env = gym.make('joinemio-v0')\r\n",
    "        self.state =  self.env.reset()\r\n",
    "        self.env.opponent_action_set(RandomPlayer.get_action)\r\n",
    "        self.env.game = gym_joinemio.envs.board.Game()\r\n",
    "        self.total_reward = 0.0\r\n",
    "\r\n",
    "    def get_action(self, board_state):\r\n",
    "        # weigths = self.net.forward(board_state)\r\n",
    "        weigths = self.net.forward(torch.flatten(torch.FloatTensor(board_state.astype(float)).type(torch.FloatTensor)))\r\n",
    "        pos_nums = self.possible_moves(board_state)\r\n",
    "        max_num = 0\r\n",
    "        for col in pos_nums:\r\n",
    "            if weigths[max_num] < weigths[int(col)]:\r\n",
    "                max_num = int(col)\r\n",
    "        return max_num\r\n",
    "\r\n",
    "    def play_step(self, net, epsilon, device):\r\n",
    "        done_reward = None\r\n",
    "        final_reward = None\r\n",
    "        if np.random.random() < epsilon:\r\n",
    "            print(\"*RANDOM PLAYER PLAYED*\")\r\n",
    "            grid = self.env.game.board.grid\r\n",
    "            action = RandomPlayer.get_action(grid)\r\n",
    "        else:\r\n",
    "            print(\"*AI PLAYER PLAYED*\")\r\n",
    "            state_a = np.array(self.state, copy=False, dtype=np.uint8)\r\n",
    "            state_v = torch.flatten(torch.FloatTensor(state_a).to(device))\r\n",
    "            q_vals_v = net(state_v)\r\n",
    "            # TODO filter\r\n",
    "            grid = self.env.game.board.grid\r\n",
    "            for i in range(Board.columns):\r\n",
    "                if i not in self.possible_moves(grid):\r\n",
    "                    q_vals_v[i] = -1\r\n",
    "            _, act_v = torch.max(q_vals_v, dim=0)\r\n",
    "            action = int(act_v.item())  # TODO: Later check if it returns correct action int range.\r\n",
    "\r\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\r\n",
    "        final_reward = reward\r\n",
    "        self.total_reward += reward.value\r\n",
    "\r\n",
    "        # exp = Transition(self.state, action, new_state, reward)\r\n",
    "        # self.replay_memory.push(exp)\r\n",
    "        self.replay_memory.push(self.state, action, new_state, reward)\r\n",
    "        self.state = new_state\r\n",
    "\r\n",
    "        if is_done == GameState.finished:\r\n",
    "            done_reward = self.total_reward\r\n",
    "            print(f\"WINNER: player {self.env.game.winner} | Final reward reading: {final_reward}\")\r\n",
    "            print(self.env.game.board.grid)\r\n",
    "            self._reset()\r\n",
    "            return (done_reward, final_reward)\r\n",
    "        else: return (done_reward, None)\r\n",
    "\r\n",
    "    def train(self, memory_buffer, batch_size):\r\n",
    "        return 0\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training loop:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def our_main():\r\n",
    "    env = gym.make('joinemio-v0')\r\n",
    "    net = NeuralNetwork().to(device)\r\n",
    "    tgt_net = NeuralNetwork().to(device)\r\n",
    "    print(net)\r\n",
    "    buffer = ReplayMemory(BUFFER_SIZE)\r\n",
    "    agent = AIPlayer(env, buffer) # TODO: params? Whatever agent is...\r\n",
    "    epsilon = EPS_START\r\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\r\n",
    "    \r\n",
    "    total_rewards = []\r\n",
    "    frame_idx = 0\r\n",
    "    ts_frame = 0\r\n",
    "    ts = time.time()\r\n",
    "    best_mean_reward = None\r\n",
    "    last_total_frames = 0\r\n",
    "    while True:\r\n",
    "        frame_idx += 1\r\n",
    "        epsilon = max(EPS_END, EPS_START - frame_idx / EPS_DECAY_LAST_FRAME)\r\n",
    "        reward, final_reward = agent.play_step(net, epsilon, device=device)\r\n",
    "        if (final_reward is not None):\r\n",
    "            frames_this_game = frame_idx - last_total_frames\r\n",
    "            last_total_frames = frame_idx\r\n",
    "            print(\"Reward is not None (Game Finished)\")\r\n",
    "            total_rewards.append(reward)\r\n",
    "            ts_frame = frame_idx\r\n",
    "            ts = time.time()\r\n",
    "            mean_reward = np.mean(total_rewards[-100:])\r\n",
    "            print(f'Total Steps Played: {frame_idx}, frames this game {frames_this_game}, Games done: {len(total_rewards)}, mean reward: {mean_reward}, eps: {epsilon}, final reward: {final_reward}')\r\n",
    "            print(\"\")          # TODO: # Neptun logging (write epsilon, speed, reward_100, reward)\r\n",
    "            \r\n",
    "            if best_mean_reward is None or best_mean_reward < mean_reward:\r\n",
    "                torch.save(net.state_dict(), \"joinemio-best.dat\") # TODO: Extract\r\n",
    "                if best_mean_reward is not None:\r\n",
    "                    print(f\"Best mean reward updated {best_mean_reward}->{mean_reward}; model saved\")\r\n",
    "                best_mean_reward = mean_reward\r\n",
    "            if mean_reward > MEAN_REWARD_BOUND and frame_idx > 10000: \r\n",
    "                print(f\"Solved in {frame_idx} frames\")\r\n",
    "                break\r\n",
    "        else: print(\"REWARD NONE\")\r\n",
    "        \r\n",
    "        if len(buffer) < REPLAY_START_SIZE:\r\n",
    "            continue\r\n",
    "\r\n",
    "        if frame_idx % SYNC_TARGET_FRAMES == 0:\r\n",
    "            tgt_net.load_state_dict(net.state_dict())\r\n",
    "\r\n",
    "        optimizer.zero_grad()\r\n",
    "        batch = buffer.sample(BATCH_SIZE)\r\n",
    "        # loss_t = calc_loss(batch, net, tgt_net, device=device) # TODO: calc_loss method\r\n",
    "        optimizer.step()\r\n",
    "\r\n",
    "our_main()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NeuralNetwork(\n",
      "  (main_layers): Sequential(\n",
      "    (0): Linear(in_features=42, out_features=42, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=42, out_features=42, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=42, out_features=42, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (output_layer): Linear(in_features=42, out_features=7, bias=True)\n",
      ")\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'ConnectFourEnv' object has no attribute 'opponent_action_set'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-71ccc374d393>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[0mour_main\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-71ccc374d393>\u001b[0m in \u001b[0;36mour_main\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mReplayMemory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBUFFER_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAIPlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# TODO: params? Whatever agent is...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEPS_START\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-f857da67e17d>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, env, replay_memory)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# self.net = NeuralNetwork()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopponent_action_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRandomPlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#self.get_action)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_memory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreplay_memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ConnectFourEnv' object has no attribute 'opponent_action_set'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have brought the network to the stage where the network learns when playing with a player making random moves. We had a problem with looping the learning process, so we used multi-armed bandit algorithm. It allowed us to break these loops. Initially, the learning player performs random moves in the same way as the opponent, but with subsequent moves, moves selected by the network are interfered more and more often. The parameter ε is responsible for the frequency of these movements. An increasingly well-trained network makes more and more non-random moves, which leads to a much larger number of won games. The network discovered that when playing with a player making random moves, the best chance of winning is by placing tokens in one column all the time.\r\n",
    "\r\n",
    "\r\n",
    "For the learning process, we used a neural network consisting of 4 layers, 42 input neurons on each of them and  7 neurons, each corresponding to the action of dropping a chip into one of 7 different columns of the board. At the input, we place a flattened board with 42 elements depicting the game boards, at the output the profitability weight of the token toss on a given selected column\r\n",
    "\r\n",
    "In a neural network, the activation function is responsible for transforming the summed weighted input from the node into the activation of the node or output for that input. The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance.\r\n",
    "\r\n",
    "We used relu functions as activation functions on our layers\r\n",
    "This function returns 0 if it receives any negative input, but for any positive value  x  it returns that value back. So it can be written as  f(x)=max(0,x)\r\n",
    "\r\n",
    "\r\n",
    "The learning process was carried out using deep q learning. ore specifically, the agents receives information on the current observation (the current state of the board) and then has to take an action (which slot to choose to add a coin). After that, nature responses with a new state and potentially yields a reward (if the game is won) or a penalty (if the game is lost or if the agent chooses an action that is not valid - such as putting a coin into an already full slot). The goal of each action is to receive the greatest possible reward. After gathering some experience, a neural network is trained to make sense of the state, action and reward relationship. The target is set such that the network aims at minimizing the loss between predicting the reward of the next_state and the realized reward.\r\n",
    "\r\n",
    "Training is nothing as iteratively playing against the trainer, memorizing what happened and updating the neural net weights after each iteration.\r\n",
    "\r\n",
    "So it looks like Deep-Q-Learning was the right choice: just by playing against a random agent, the neural network was trained to win the game - even without knowing the rules first!\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "8d71e197be79a8854e36fe80d7b81a808575f897a047ec71b5b71be66361356a"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}